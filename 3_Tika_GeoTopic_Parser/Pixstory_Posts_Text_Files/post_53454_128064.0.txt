How are social media platforms managing vaccine misinformation at this stage in the pandemic?

Anti-vaccine sentiment has been building since 2020, and hasn’t gone anywhere. In fact, it will have intensified following the recent approval of COVID-19 vaccinations for some babies and children under five, and the recommendation for a fourth booster shot for people over 30.

And although anti-vaxxers can be found in most online spaces, Facebook has historically been one of their platforms of choice.

Swinburne PhD student Damilola Ayeni has been interviewing anti-vaccine activists since 2019, to learn about how they grow their audience on Facebook and how they evade moderation.

Her findings help shed light on the tug-of-war between Facebook’s content moderation efforts and an unrelenting slew of vaccine misinformation.

What’s been happening?

Facebook has been moderating content under the COVID-19 and vaccine policy. It does this by warning group admins and moderators, deleting offending accounts or groups, and flagging posts containing misinformation.

In its first response to Australia’s DIGI Misinformation and Disinformation Code, Facebook said it had “removed over 14 million pieces of content that constituted misinformation related to COVID-19” – of which 110,000 were from Australian pages.

Despite this, Facebook’s moderation approach has loopholes that anti-vaxxers continue to exploit. For instance, the ABC recently fact-checked anti-vaxxers who were spreading misinformation on Facebook by claiming COVID-19 vaccines were responsible for the sudden death of a Queensland toddler.

Ayeni’s research found anti-vax Facebook groups are now “self-moderating”. This means they predict what Facebook’s automated moderation tools and independent fact-checkers will be looking for, and change their posting techniques accordingly.

Read more: https://theconversation.com/a-researcher-asked-covid-anti-vaxxers-how-they-avoid-facebook-moderation-heres-what-they-found-186406