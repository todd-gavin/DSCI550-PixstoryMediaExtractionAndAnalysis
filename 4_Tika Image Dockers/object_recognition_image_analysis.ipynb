{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tika\n",
    "import tika.parser\n",
    "from tika import parser\n",
    "import requests\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run \"python -mhttp.server 8888\" in the directory where your photos are\n",
    "\n",
    "#run ifconfig in terminal to find your ip address listed for en0\n",
    "\n",
    "images_url_usc  = 'http://10.25.179.208:8888'\n",
    "\n",
    "images_url_usc2 = 'http://10.25.77.81:8888'\n",
    "\n",
    "images_url_home = 'http://192.168.1.13:8888'\n",
    "\n",
    "images_url_MOC = 'http://10.25.188.76:8888'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping all 80,581 URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All links processed!s\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "webpage_url = images_url_home\n",
    "\n",
    "response = requests.get(webpage_url)\n",
    "html_content = response.content\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "image_urls_set = set()\n",
    "count = 0\n",
    "for link in soup.find_all('a'):\n",
    "    href = link.get('href')\n",
    "    if href is not None and href.startswith('http'):\n",
    "        image_urls_set.add(href)\n",
    "        count += 1\n",
    "    elif href is not None:\n",
    "        image_urls_set.add(webpage_url + '/' + href)\n",
    "        count += 1\n",
    "    print(f\"Processed {count} links\", end='\\r')\n",
    "\n",
    "image_urls = list(image_urls_set)\n",
    "\n",
    "# Save the image URLs to a text file\n",
    "with open('image_urls.txt', mode='w') as file:\n",
    "    for url in image_urls:\n",
    "        file.write(url + '\\n')\n",
    "        \n",
    "print(\"All links processed!\") # added for confirmation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains 80581 links.\n",
      "The file contains 80581 unique links.\n"
     ]
    }
   ],
   "source": [
    "#verifying there were no mistakes durnig link extraction\n",
    "with open('image_urls.txt', 'r') as f:\n",
    "    num_links = len(f.readlines())\n",
    "\n",
    "print(f'The file contains {num_links} links.')\n",
    "\n",
    "with open('image_urls.txt', 'r') as f:\n",
    "    unique_links = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "num_unique_links = len(unique_links)\n",
    "\n",
    "print(f'The file contains {num_unique_links} unique links.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Object Recognition JSON file creation utilizing threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `docker run -it -p 8764:8764 uscdatascience/inception-rest-tika`\n",
    "- test_url_objects = 'http://localhost:8764/inception/v4/classify/image?topn=2&min_confidence=0.03&url=http://192.168.1.13:8888/Pixstory-image-164318304274781.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 images to file image_object_recognition_data4.json.\n",
      "Completed Chunk 0.\n",
      "Image URL List Current Index: 100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/daniilabbruzzese/Documents/Senior Year/DSCI 550/assignment 2/DSCI550-PixstoryMediaExtractionAndAnalysis/4_Tika Image Dockers/object_recognition_image_analysis.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniilabbruzzese/Documents/Senior%20Year/DSCI%20550/assignment%202/DSCI550-PixstoryMediaExtractionAndAnalysis/4_Tika%20Image%20Dockers/object_recognition_image_analysis.ipynb#W6sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     t\u001b[39m.\u001b[39mstart()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniilabbruzzese/Documents/Senior%20Year/DSCI%20550/assignment%202/DSCI550-PixstoryMediaExtractionAndAnalysis/4_Tika%20Image%20Dockers/object_recognition_image_analysis.ipynb#W6sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# wait for all images to be processed\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/daniilabbruzzese/Documents/Senior%20Year/DSCI%20550/assignment%202/DSCI550-PixstoryMediaExtractionAndAnalysis/4_Tika%20Image%20Dockers/object_recognition_image_analysis.ipynb#W6sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m image_url_queue\u001b[39m.\u001b[39;49mjoin()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/queue.py:90\u001b[0m, in \u001b[0;36mQueue.join\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_tasks_done:\n\u001b[1;32m     89\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munfinished_tasks:\n\u001b[0;32m---> 90\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_tasks_done\u001b[39m.\u001b[39;49mwait()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import queue\n",
    "import time\n",
    "import json\n",
    "\n",
    "# cnt = 0\n",
    "\n",
    "# specify the starting index for processing images\n",
    "# start_index = 0\n",
    "\n",
    "# create a queue to hold the image URLs\n",
    "image_url_queue = queue.Queue()\n",
    "\n",
    "# populate the queue with the image URLs starting from the specified index\n",
    "for image_url in image_urls[0:]:\n",
    "    image_url_queue.put(image_url)\n",
    "\n",
    "# create a list to hold the processed image data\n",
    "image_object_recognition_data = []\n",
    "\n",
    "# Set starting index\n",
    "start_index = 0\n",
    "\n",
    "# remove the top items from the queue based \n",
    "for i in range(start_index):\n",
    "    image_url_queue.queue.popleft()\n",
    "\n",
    "# Creating file name to hold image captions data\n",
    "image_object_recogntion_JSON_data = 'image_object_recognition_data_Master.json'\n",
    "\n",
    "def process_images():\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # get the next image URL from the queue\n",
    "        image_url = image_url_queue.get()\n",
    "\n",
    "        # construct the API URL for the image classification\n",
    "        # api_captions_url = 'http://localhost:8764/inception/v3/caption/image?url=' + image_url\n",
    "        api_objects_url = 'http://localhost:8764/inception/v4/classify/image?topn=2&min_confidence=0.03&url=' + image_url\n",
    "\n",
    "        # send the API request\n",
    "        response = requests.get(api_objects_url)\n",
    "\n",
    "        # parse the JSON response\n",
    "        json_response = response.json()\n",
    "        # print(json_response)\n",
    "\n",
    "        # Extract the top two labels from the JSON response\n",
    "        classnames = json_response['classnames']\n",
    "\n",
    "        # create a dictionary to store the image URL and its caption\n",
    "        data = {'url': image_url, 'object_recognition': classnames}\n",
    "        # print(data)\n",
    "\n",
    "        # add the dictionary to the list of image data\n",
    "        image_object_recognition_data.append(data)\n",
    "\n",
    "        # save the image data to disk periodically\n",
    "        if len(image_object_recognition_data) % 100 == 0:\n",
    "            with open(image_object_recogntion_JSON_data, 'w') as f:\n",
    "                json.dump(image_object_recognition_data, f)\n",
    "            print(f'Saved {len(image_object_recognition_data)} images to file {image_object_recogntion_JSON_data}.\\nCompleted Chunk {int(len(image_object_recognition_data)/100) + (int(start_index/100)-1)}.\\nImage URL List Current Index: {int(len(image_object_recognition_data)) + int(start_index)}', flush=True)\n",
    "\n",
    "        # mark the image URL as processed\n",
    "        image_url_queue.task_done()\n",
    "\n",
    "# create a pool of worker threads to process the images\n",
    "num_threads = 20\n",
    "for i in range(num_threads):\n",
    "    t = threading.Thread(target=process_images)\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "# wait for all images to be processed\n",
    "image_url_queue.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The JSON object contains 118 image captions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Saved image data to disk\n",
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Processed 20 images\n",
      "Processed 20 images\n"
     ]
    }
   ],
   "source": [
    "# count the number of dictionaries in the list\n",
    "num_image_captions = len(image_object_recognition_data)\n",
    "\n",
    "print(f'The JSON object contains {num_image_captions} image captions.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14b5f4a1e35b3543103fc71a08ac87c31dbc262e66f163b05b7ba3f77eb1e8a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
